{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import findspark\r\n",
    "findspark.init()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pyspark\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.types import *\r\n",
    "from pyspark.sql.functions import *\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Configur spark session\r\n",
    "spark = SparkSession\\\r\n",
    "    .builder\\\r\n",
    "    .master('local[4]')\\\r\n",
    "    .appName('quake_etl')\\\r\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:2.4.1')\\\r\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Loading dataset\r\n",
    "df_load = spark.read.csv(r\"C:\\1_My_things\\1. Sem 7\\BDAD\\database.csv\", header = True)\r\n",
    "# Preview dr_load\r\n",
    "df_load.take(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(Date='01/02/1965', Time='13:44:18', Latitude='19.246', Longitude='145.616', Type='Earthquake', Depth='131.6', Depth Error=None, Depth Seismic Stations=None, Magnitude='6', Magnitude Type='MW', Magnitude Error=None, Magnitude Seismic Stations=None, Azimuthal Gap=None, Horizontal Distance=None, Horizontal Error=None, Root Mean Square=None, ID='ISCGEM860706', Source='ISCGEM', Location Source='ISCGEM', Magnitude Source='ISCGEM', Status='Automatic')]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "lst_dropped_columns = [\r\n",
    "    'Depth Error',\r\n",
    "    'Time',\r\n",
    "    'Depth Seismic Stations',\r\n",
    "    'Magnitude Error',\r\n",
    "    'Magnitude Seismic Stations',\r\n",
    "    'Azimuthal Gap',\r\n",
    "    'Horizontal Distance',\r\n",
    "    'Horizontal Error',\r\n",
    "    'Root Mean Square',\r\n",
    "    'Source',\r\n",
    "    'Location Source',\r\n",
    "    'Magnitude Source',\r\n",
    "    'Status'\r\n",
    "]\r\n",
    "\r\n",
    "# Drop all the columns listed above and assign new dataframe to df_drop\r\n",
    "df_load = df_load.drop(*lst_dropped_columns)\r\n",
    "\r\n",
    "# Preview df_drop\r\n",
    "df_load.show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+---------------+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|             ID|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+---------------+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|        6|            MW|   ISCGEM860706|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake|   80|      5.8|            MW|   ISCGEM860737|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake|   20|      6.2|            MW|   ISCGEM860762|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake|   15|      5.8|            MW|   ISCGEM860856|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake|   15|      5.8|            MW|   ISCGEM860890|\n",
      "|01/10/1965| -13.405|  166.629|Earthquake|   35|      6.7|            MW|   ISCGEM860922|\n",
      "|01/12/1965|  27.357|   87.867|Earthquake|   20|      5.9|            MW|   ISCGEM861007|\n",
      "|01/15/1965| -13.309|  166.212|Earthquake|   35|        6|            MW|   ISCGEM861111|\n",
      "|01/16/1965| -56.452|  -27.043|Earthquake|   95|        6|            MW|ISCGEMSUP861125|\n",
      "|01/17/1965| -24.563|  178.487|Earthquake|  565|      5.8|            MW|   ISCGEM861148|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Creating year field and add it to the dataframe\r\n",
    "df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\r\n",
    "# preview df_load\r\n",
    "df_load.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|        6|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake|   80|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake|   20|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake|   15|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake|   15|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Build the quakes frequiency dataframe using year filed and count for each year\r\n",
    "df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\r\n",
    "# Preview df_quake_freq\r\n",
    "df_quake_freq.show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+------+\n",
      "|Year|Counts|\n",
      "+----+------+\n",
      "|1990|   196|\n",
      "|1975|   150|\n",
      "|1977|   148|\n",
      "|2003|   187|\n",
      "|2007|   211|\n",
      "|1974|   147|\n",
      "|2015|   175|\n",
      "|2006|   176|\n",
      "|1978|   158|\n",
      "|2013|   202|\n",
      "+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Preview df_load schema\r\n",
    "df_load.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: string (nullable = true)\n",
      " |-- Magnitude: string (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# cast some fielts from string into numeric types\r\n",
    "df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\r\n",
    "    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\r\n",
    "    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\r\n",
    "    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\r\n",
    "# preview df_load\r\n",
    "df_load.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# preview df_load schema\r\n",
    "df_load.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# creating avg magnitude and max magnitude fields and add to df_quake_feq\r\n",
    "df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\r\n",
    "df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')\r\n",
    "df_max.show(5)\r\n",
    "df_avg.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------------+\n",
      "|Year|Max_Magnitude|\n",
      "+----+-------------+\n",
      "|1990|          7.6|\n",
      "|1975|          7.8|\n",
      "|1977|          7.6|\n",
      "|2003|          7.6|\n",
      "|2007|          8.4|\n",
      "+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-----------------+\n",
      "|Year|    Avg_Magnitude|\n",
      "+----+-----------------+\n",
      "|1990|5.858163265306125|\n",
      "|1975| 5.84866666666667|\n",
      "|1977|5.757432432432437|\n",
      "|2003|5.850802139037435|\n",
      "|2007| 5.89099526066351|\n",
      "+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# joining df_max and df_avg to main df_quake_freq\r\n",
    "df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\r\n",
    "df_quake_freq.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+------+-----------------+-------------+\n",
      "|Year|Counts|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+------+-----------------+-------------+\n",
      "|1990|   196|5.858163265306125|          7.6|\n",
      "|1975|   150| 5.84866666666667|          7.8|\n",
      "|1977|   148|5.757432432432437|          7.6|\n",
      "|2003|   187|5.850802139037435|          7.6|\n",
      "|2007|   211| 5.89099526066351|          8.4|\n",
      "+----+------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/Quake.quakes?readPreference=primaryPreferred\" --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/Quake.quakes\" --packages org.mongodb.spark:mongo-spark-connector_2.12:2.4.1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Removing null values\r\n",
    "df_load.dropna()\r\n",
    "df_quake_freq.dropna()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Counts: bigint, Avg_Magnitude: double, Max_Magnitude: double]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Preview data frames df_load\r\n",
    "df_load.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Preview dataframe df_quake_freq\r\n",
    "df_quake_freq.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+------+-----------------+-------------+\n",
      "|Year|Counts|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+------+-----------------+-------------+\n",
      "|1990|   196|5.858163265306125|          7.6|\n",
      "|1975|   150| 5.84866666666667|          7.8|\n",
      "|1977|   148|5.757432432432437|          7.6|\n",
      "|2003|   187|5.850802139037435|          7.6|\n",
      "|2007|   211| 5.89099526066351|          8.4|\n",
      "+----+------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# building collection in mongodb\r\n",
    "# Write df_load to mongodb\r\n",
    "# df_load.write.format('mongo')\\\r\n",
    "#     .mode('overwrite')\\\r\n",
    "#     .option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quakes').save()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# write df_quake_freq to mongodb\r\n",
    "# df_quake_freq.write.format('mongo')\\\r\n",
    "#     .mode('overwrite')\\\r\n",
    "#     .option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quake_freq').save()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Data Pre-Processing</center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Loading test data file into a dataframe\r\n",
    "df_test = spark.read.csv(r\"C:\\1_My_things\\1. Sem 7\\BDAD\\query.csv\", header=True)\r\n",
    "# preview df_test\r\n",
    "df_test.take(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(time='2017-01-02T00:13:06.300Z', latitude='-36.0365', longitude='51.9288', depth='10', mag='5.7', magType='mwb', nst=None, gap='26', dmin='14.685', rms='1.37', net='us', id='us10007p5d', updated='2017-03-27T23:53:17.040Z', place='Southwest Indian Ridge', type='earthquake', horizontalError='10.3', depthError='1.7', magError='0.068', magNst='21', status='reviewed', locationSource='us', magSource='us'),\n",
       " Row(time='2017-01-02T13:13:48.710Z', latitude='-4.895', longitude='-76.3675', depth='106', mag='5.9', magType='mww', nst=None, gap='31', dmin='3.002', rms='0.82', net='us', id='us10007p7n', updated='2020-01-02T23:56:16.978Z', place='37km E of Barranca, Peru', type='earthquake', horizontalError='7.1', depthError='1.9', magError=None, magNst=None, status='reviewed', locationSource='us', magSource='us'),\n",
       " Row(time='2017-01-02T13:14:02.830Z', latitude='-23.2513', longitude='179.2383', depth='551.62', mag='6.3', magType='mww', nst=None, gap='36', dmin='5.59', rms='0.88', net='us', id='us10007p7m', updated='2017-03-27T23:53:18.040Z', place='South of the Fiji Islands', type='earthquake', horizontalError='9.9', depthError='3.3', magError='0.05', magNst='38', status='reviewed', locationSource='us', magSource='us'),\n",
       " Row(time='2017-01-03T09:09:02.080Z', latitude='24.0151', longitude='92.0177', depth='32', mag='5.7', magType='mww', nst=None, gap='20', dmin='1.375', rms='0.86', net='us', id='us10007pfk', updated='2020-01-02T23:56:29.214Z', place='20km ENE of Ambasa, India', type='earthquake', horizontalError='6.7', depthError='1.8', magError='0.071', magNst='19', status='reviewed', locationSource='us', magSource='us'),\n",
       " Row(time='2017-01-03T21:19:07.540Z', latitude='-43.3527', longitude='-74.5017', depth='10.26', mag='5.5', magType='mww', nst=None, gap='96', dmin='0.656', rms='0.91', net='us', id='us10007piy', updated='2020-01-02T23:56:57.327Z', place='76km WSW of Puerto Quellon, Chile', type='earthquake', horizontalError='4.1', depthError='1.7', magError='0.071', magNst='19', status='reviewed', locationSource='us', magSource='us')]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Loading the training data from mongodb into a dataframe\r\n",
    "df_train = spark.read.format(\"mongo\")\\\r\n",
    "    .option('spark.mongodb.input.uri', 'mongodb://127.0.0.1:27017/Quake.quakes').load()\r\n",
    "\r\n",
    "# Preview df_train\r\n",
    "df_train.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "|      Date|Depth|          ID|Latitude|Longitude|Magnitude|Magnitude Type|      Type|Year|                 _id|\n",
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "|01/02/1965|131.6|ISCGEM860706|  19.246|  145.616|      6.0|            MW|Earthquake|1965|{6118c048fb7a112a...|\n",
      "|01/04/1965| 80.0|ISCGEM860737|   1.863|  127.352|      5.8|            MW|Earthquake|1965|{6118c048fb7a112a...|\n",
      "|01/05/1965| 20.0|ISCGEM860762| -20.579| -173.972|      6.2|            MW|Earthquake|1965|{6118c048fb7a112a...|\n",
      "|01/08/1965| 15.0|ISCGEM860856| -59.076|  -23.557|      5.8|            MW|Earthquake|1965|{6118c048fb7a112a...|\n",
      "|01/09/1965| 15.0|ISCGEM860890|  11.938|  126.427|      5.8|            MW|Earthquake|1965|{6118c048fb7a112a...|\n",
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# selecting usful columns from df_train\r\n",
    "df_test_clean = df_test['time', 'latitude', 'longitude', 'mag', 'depth']\r\n",
    "# Preview df_test_clean\r\n",
    "df_test_clean.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------+---------+---+------+\n",
      "|                time|latitude|longitude|mag| depth|\n",
      "+--------------------+--------+---------+---+------+\n",
      "|2017-01-02T00:13:...|-36.0365|  51.9288|5.7|    10|\n",
      "|2017-01-02T13:13:...|  -4.895| -76.3675|5.9|   106|\n",
      "|2017-01-02T13:14:...|-23.2513| 179.2383|6.3|551.62|\n",
      "|2017-01-03T09:09:...| 24.0151|  92.0177|5.7|    32|\n",
      "|2017-01-03T21:19:...|-43.3527| -74.5017|5.5| 10.26|\n",
      "+--------------------+--------+---------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Rename fields\r\n",
    "df_test_clean = df_test_clean.withColumnRenamed('time', 'Date')\\\r\n",
    "    .withColumnRenamed('latitude', 'Latitude')\\\r\n",
    "    .withColumnRenamed('longitude', 'Longitude')\\\r\n",
    "    .withColumnRenamed('mag', 'Magnitude')\\\r\n",
    "    .withColumnRenamed('depth', 'Depth')\r\n",
    "\r\n",
    "# Preview df_test_clean\r\n",
    "df_test_clean.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------+---------+---------+------+\n",
      "|                Date|Latitude|Longitude|Magnitude| Depth|\n",
      "+--------------------+--------+---------+---------+------+\n",
      "|2017-01-02T00:13:...|-36.0365|  51.9288|      5.7|    10|\n",
      "|2017-01-02T13:13:...|  -4.895| -76.3675|      5.9|   106|\n",
      "|2017-01-02T13:14:...|-23.2513| 179.2383|      6.3|551.62|\n",
      "|2017-01-03T09:09:...| 24.0151|  92.0177|      5.7|    32|\n",
      "|2017-01-03T21:19:...|-43.3527| -74.5017|      5.5| 10.26|\n",
      "+--------------------+--------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Preview df_test_clean schema\r\n",
    "df_test_clean.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Magnitude: string (nullable = true)\n",
      " |-- Depth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# cast some fields from string into numeric types\r\n",
    "df_test_clean = df_test_clean.withColumn('Latitude', df_test_clean['Latitude'].cast(DoubleType()))\\\r\n",
    "    .withColumn('Longitude', df_test_clean['Longitude'].cast(DoubleType()))\\\r\n",
    "    .withColumn('Depth', df_test_clean['Depth'].cast(DoubleType()))\\\r\n",
    "    .withColumn('Magnitude', df_test_clean['Magnitude'].cast(DoubleType()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "df_test_clean.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Create training and test dataframes\r\n",
    "df_testing = df_test_clean['Latitude', 'Longitude', 'Magnitude', 'Depth']\r\n",
    "df_training = df_train['Latitude', 'Longitude', 'Magnitude', 'Depth']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# preview df_testing and df_training\r\n",
    "df_testing.show(5)\r\n",
    "df_training.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+---------+------+\n",
      "|Latitude|Longitude|Magnitude| Depth|\n",
      "+--------+---------+---------+------+\n",
      "|-36.0365|  51.9288|      5.7|  10.0|\n",
      "|  -4.895| -76.3675|      5.9| 106.0|\n",
      "|-23.2513| 179.2383|      6.3|551.62|\n",
      "| 24.0151|  92.0177|      5.7|  32.0|\n",
      "|-43.3527| -74.5017|      5.5| 10.26|\n",
      "+--------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+---------+---------+-----+\n",
      "|Latitude|Longitude|Magnitude|Depth|\n",
      "+--------+---------+---------+-----+\n",
      "|  19.246|  145.616|      6.0|131.6|\n",
      "|   1.863|  127.352|      5.8| 80.0|\n",
      "| -20.579| -173.972|      6.2| 20.0|\n",
      "| -59.076|  -23.557|      5.8| 15.0|\n",
      "|  11.938|  126.427|      5.8| 15.0|\n",
      "+--------+---------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Drop redords from df_training and df_testing which are having null values\r\n",
    "df_training = df_training.dropna()\r\n",
    "df_testing = df_testing.dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from pyspark.ml import Pipeline\r\n",
    "from pyspark.ml.regression import RandomForestRegressor\r\n",
    "from pyspark.ml.feature import VectorAssembler\r\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# select features to parse into our model and then create the feature vactor\r\n",
    "assembler = VectorAssembler(inputCols=['Latitude', 'Longitude', 'Depth'], outputCol='features')\r\n",
    "\r\n",
    "# Creating the model\r\n",
    "model_reg = RandomForestRegressor(featuresCol='features', labelCol='Magnitude')\r\n",
    "\r\n",
    "# Chain the assembler and the model in a Pipeline\r\n",
    "pipeline = Pipeline(stages=[assembler, model_reg])\r\n",
    "\r\n",
    "# Train the model\r\n",
    "model = pipeline.fit(df_training)\r\n",
    "\r\n",
    "# Make predictions\r\n",
    "pred_results = model.transform(df_testing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Preview pred_results dataframe\r\n",
    "pred_results.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+---------+------+--------------------+------------------+\n",
      "|Latitude|Longitude|Magnitude| Depth|            features|        prediction|\n",
      "+--------+---------+---------+------+--------------------+------------------+\n",
      "|-36.0365|  51.9288|      5.7|  10.0|[-36.0365,51.9288...| 5.823307350071318|\n",
      "|  -4.895| -76.3675|      5.9| 106.0|[-4.895,-76.3675,...|5.8744483379578565|\n",
      "|-23.2513| 179.2383|      6.3|551.62|[-23.2513,179.238...|5.9122662680567535|\n",
      "| 24.0151|  92.0177|      5.7|  32.0|[24.0151,92.0177,...| 5.878633114919993|\n",
      "|-43.3527| -74.5017|      5.5| 10.26|[-43.3527,-74.501...| 5.868048428536131|\n",
      "+--------+---------+---------+------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Evaluate the model\r\n",
    "# rmse should be less the 0.5 for the model to the useful\r\n",
    "evaluator = RegressionEvaluator(labelCol='Magnitude', predictionCol='prediction', metricName='rmse')\r\n",
    "rmse = evaluator.evaluate(pred_results)\r\n",
    "print(\"Root Main Squared Error (RMSE) on test data = %g\" % rmse)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Root Main Squared Error (RMSE) on test data = 0.402147\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Predictive Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Creating the predictive dataset\r\n",
    "df_pred_results = pred_results['Latitude', 'Longitude', 'prediction']\r\n",
    "\r\n",
    "# Rename the prediction field\r\n",
    "df_pred_results = df_pred_results.withColumnRenamed('prediction', 'Pred_Magnitude')\r\n",
    "\r\n",
    "# Add more columns to our prediction dataset\r\n",
    "df_pred_results = df_pred_results.withColumn('Year', lit(2017))\\\r\n",
    "    .withColumn('RSME', lit(rmse))\r\n",
    "\r\n",
    "# Preview df_pred_results\r\n",
    "df_pred_results.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+------------------+----+------------------+\n",
      "|Latitude|Longitude|    Pred_Magnitude|Year|              RSME|\n",
      "+--------+---------+------------------+----+------------------+\n",
      "|-36.0365|  51.9288| 5.823307350071318|2017|0.4021472125398876|\n",
      "|  -4.895| -76.3675|5.8744483379578565|2017|0.4021472125398876|\n",
      "|-23.2513| 179.2383|5.9122662680567535|2017|0.4021472125398876|\n",
      "| 24.0151|  92.0177| 5.878633114919993|2017|0.4021472125398876|\n",
      "|-43.3527| -74.5017| 5.868048428536131|2017|0.4021472125398876|\n",
      "+--------+---------+------------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "127476f26a63e7b82ab42f7db8244ce309f5a90dbe7da8e80772de56a956c678"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}